{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g_iiBRSPt2l"
      },
      "outputs": [],
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import timm\n",
        "from einops import rearrange\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqt3v4hkPxoO",
        "outputId": "cd689eb0-83ac-4615-b72b-1e132b7ddbfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2026-02-05 05:21:46--  https://www.dropbox.com/scl/fi/gp34duk5wdqea9fceys6v/scenario17.zip?rlkey=5758v2ne70qyddg7c8505ufv5&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uca714b1b029360628f417da3472.dl.dropboxusercontent.com/cd/0/inline/C6Rx3tA5we7M8uhkpFJT4la6ZnEcePjtPSVdyYyX-uJomkfPGNCU2hT1UDv373T2QnbuMhYWLjksHChOdJ5JAMqBph7EtdpQvhYMEXbr8xnogtiq9VEkCz-TBAAk4TaoLvTHiOwo1tyF649MqgxufSUZ/file?dl=1# [following]\n",
            "--2026-02-05 05:21:47--  https://uca714b1b029360628f417da3472.dl.dropboxusercontent.com/cd/0/inline/C6Rx3tA5we7M8uhkpFJT4la6ZnEcePjtPSVdyYyX-uJomkfPGNCU2hT1UDv373T2QnbuMhYWLjksHChOdJ5JAMqBph7EtdpQvhYMEXbr8xnogtiq9VEkCz-TBAAk4TaoLvTHiOwo1tyF649MqgxufSUZ/file?dl=1\n",
            "Resolving uca714b1b029360628f417da3472.dl.dropboxusercontent.com (uca714b1b029360628f417da3472.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uca714b1b029360628f417da3472.dl.dropboxusercontent.com (uca714b1b029360628f417da3472.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/C6RfqTe4Z8A_okPeF9Q6KTf0khdmhUZ_WN_UysDaSI9A9lChkiBUecKu8vCzTwzx33cdQFDz0jNdchxrggX47b4oJp1X4vv1patMHdcND4CRouj6K7mxTNRQfZudQrbHuM8mwo2FN8eLvKD7wqA14-K_s0JhcRIyA5Qh_KAjB5ZSGrPlbgcxAVHiT-EF4ECP8hVq4708I5BJRqwvggG8-lv2Znn9MEXTjDpKoTJs3qqN75vM2cTvW_-tCogAodJdFsLstzkj2durMiYIPglwB-yDXIiKPf3DrfsSqdpvNbiYgMGzDErICl5rC2JpAH_DztW3JWkoCvURa8DNJ33tXQspWmt2ceQ4P-M3Zxr8woNs2IrKdS60kVJ0CdOHJ8E3LmM/file?dl=1 [following]\n",
            "--2026-02-05 05:21:48--  https://uca714b1b029360628f417da3472.dl.dropboxusercontent.com/cd/0/inline2/C6RfqTe4Z8A_okPeF9Q6KTf0khdmhUZ_WN_UysDaSI9A9lChkiBUecKu8vCzTwzx33cdQFDz0jNdchxrggX47b4oJp1X4vv1patMHdcND4CRouj6K7mxTNRQfZudQrbHuM8mwo2FN8eLvKD7wqA14-K_s0JhcRIyA5Qh_KAjB5ZSGrPlbgcxAVHiT-EF4ECP8hVq4708I5BJRqwvggG8-lv2Znn9MEXTjDpKoTJs3qqN75vM2cTvW_-tCogAodJdFsLstzkj2durMiYIPglwB-yDXIiKPf3DrfsSqdpvNbiYgMGzDErICl5rC2JpAH_DztW3JWkoCvURa8DNJ33tXQspWmt2ceQ4P-M3Zxr8woNs2IrKdS60kVJ0CdOHJ8E3LmM/file?dl=1\n",
            "Reusing existing connection to uca714b1b029360628f417da3472.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13737306249 (13G) [application/binary]\n",
            "Saving to: ‘scenario17.zip’\n",
            "\n",
            "scenario17.zip      100%[===================>]  12.79G  81.8MB/s    in 2m 59s  \n",
            "\n",
            "2026-02-05 05:24:47 (73.0 MB/s) - ‘scenario17.zip’ saved [13737306249/13737306249]\n",
            "\n",
            "   index                                     unit1_rgb  \\\n",
            "0      1  ./unit1/camera_data/image_BS2_1_23_59_00.jpg   \n",
            "1      2  ./unit1/camera_data/image_BS2_2_23_59_00.jpg   \n",
            "2      3  ./unit1/camera_data/image_BS2_3_23_59_00.jpg   \n",
            "3      4  ./unit1/camera_data/image_BS2_4_23_59_00.jpg   \n",
            "4      5  ./unit1/camera_data/image_BS2_5_23_59_00.jpg   \n",
            "\n",
            "                          unit1_pwr_60ghz                  unit1_blockage  \\\n",
            "0  ./unit1/mmWave_data/mmWave_power_1.txt  ./unit1/label_data/label_1.txt   \n",
            "1  ./unit1/mmWave_data/mmWave_power_2.txt  ./unit1/label_data/label_2.txt   \n",
            "2  ./unit1/mmWave_data/mmWave_power_3.txt  ./unit1/label_data/label_3.txt   \n",
            "3  ./unit1/mmWave_data/mmWave_power_4.txt  ./unit1/label_data/label_4.txt   \n",
            "4  ./unit1/mmWave_data/mmWave_power_5.txt  ./unit1/label_data/label_5.txt   \n",
            "\n",
            "                           unit1_loc                          unit2_loc  \\\n",
            "0  ./unit1/GPS_data/gps_location.txt  ./unit2/GPS_data/gps_location.txt   \n",
            "1  ./unit1/GPS_data/gps_location.txt  ./unit2/GPS_data/gps_location.txt   \n",
            "2  ./unit1/GPS_data/gps_location.txt  ./unit2/GPS_data/gps_location.txt   \n",
            "3  ./unit1/GPS_data/gps_location.txt  ./unit2/GPS_data/gps_location.txt   \n",
            "4  ./unit1/GPS_data/gps_location.txt  ./unit2/GPS_data/gps_location.txt   \n",
            "\n",
            "   seq_index   time_stamp[UTC]  \n",
            "0          1    ['23-59-00-0']  \n",
            "1          1   ['23-59-00-90']  \n",
            "2          1  ['23-59-00-180']  \n",
            "3          1  ['23-59-00-270']  \n",
            "4          1  ['23-59-00-360']  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Direct download link\n",
        "url = \"https://www.dropbox.com/scl/fi/gp34duk5wdqea9fceys6v/scenario17.zip?rlkey=5758v2ne70qyddg7c8505ufv5&dl=1\"\n",
        "\n",
        "# Download the zip file\n",
        "!wget -O scenario17.zip \"$url\"\n",
        "\n",
        "# Unzip the contents\n",
        "with zipfile.ZipFile(\"scenario17.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"scenario17\")\n",
        "\n",
        "# List extracted files\n",
        "os.listdir(\"scenario17\")\n",
        "\n",
        "data = pd.read_csv(\"/content/scenario17/scenario17.csv\")\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fRx6-eXJPd_S",
        "outputId": "dfcccfed-43e8-4044-8324-8c0eaadca695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading CSV...\n",
            "label\n",
            "0    68551\n",
            "1     1449\n",
            "Name: count, dtype: int64\n",
            "Total windows: 69928\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1894613532.py:339: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/amp/grad_scaler.py:31: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  super().__init__(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training (FAST MODE)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/6993 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1894613532.py:183: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  torch.tensor(imgs,dtype=torch.float32),\n",
            "/tmp/ipython-input-1894613532.py:183: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  torch.tensor(imgs,dtype=torch.float32),\n",
            "/tmp/ipython-input-1894613532.py:358: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "  7%|▋         | 508/6993 [2:20:35<29:54:51, 16.61s/it]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\", line 212, in collate\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\", line 271, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Trying to resize storage that is not resizable\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1894613532.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1504\u001b[0m                 \u001b[0mworker_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1539\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# be constructed, don't try to instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\", line 212, in collate\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\", line 271, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Trying to resize storage that is not resizable\n"
          ]
        }
      ],
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import timm\n",
        "from einops import rearrange\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================\n",
        "# FINAL PIPELINE\n",
        "# FAST + MULTI-TASK + XAI\n",
        "# ViT + Signal Transformer + Attention\n",
        "# Horizons: 1,5,10\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# CONFIG\n",
        "# ============================================================\n",
        "\n",
        "BASE = \"/content/scenario17\"\n",
        "CACHE = \"cache_multi_fast\"\n",
        "\n",
        "BATCH = 8\n",
        "EPOCHS = 5\n",
        "LR = 1e-4\n",
        "WORKERS = 2\n",
        "\n",
        "SEQ = 8\n",
        "HORIZONS = [1,5,10]\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "os.makedirs(CACHE, exist_ok=True)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1. LOAD DATA\n",
        "# ============================================================\n",
        "\n",
        "print(\"Loading CSV...\")\n",
        "\n",
        "df = pd.read_csv(\"/content/scenario17/scenario17.csv\")\n",
        "\n",
        "\n",
        "def fix(p):\n",
        "    return os.path.join(BASE, p.replace(\"./\", \"\"))\n",
        "\n",
        "\n",
        "for c in [\"unit1_rgb\",\"unit1_pwr_60ghz\",\"unit1_blockage\"]:\n",
        "    df[c] = df[c].apply(fix)\n",
        "\n",
        "\n",
        "def read_label(p):\n",
        "    with open(p) as f:\n",
        "        return int(float(f.readline()))\n",
        "\n",
        "\n",
        "df[\"label\"] = df[\"unit1_blockage\"].apply(read_label)\n",
        "\n",
        "print(df[\"label\"].value_counts())\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2. WINDOW BUILDER (MULTI-HORIZON)\n",
        "# ============================================================\n",
        "\n",
        "def build_windows(df):\n",
        "\n",
        "    windows = []\n",
        "\n",
        "    if \"seq_index\" in df.columns:\n",
        "        groups = df.groupby(\"seq_index\")\n",
        "    else:\n",
        "        groups = [(0,df)]\n",
        "\n",
        "    for _,g in groups:\n",
        "\n",
        "        g = g.reset_index(drop=True)\n",
        "\n",
        "        i = 0\n",
        "\n",
        "        while i+SEQ+max(HORIZONS) < len(g):\n",
        "\n",
        "            win = g.iloc[i:i+SEQ]\n",
        "\n",
        "            labels = [\n",
        "                g.iloc[i+SEQ-1+h][\"label\"]\n",
        "                for h in HORIZONS\n",
        "            ]\n",
        "\n",
        "            windows.append((win,labels))\n",
        "\n",
        "            i+=1\n",
        "\n",
        "    print(\"Total windows:\",len(windows))\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "windows = build_windows(df)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3. CACHED DATASET\n",
        "# ============================================================\n",
        "\n",
        "def load_signal(p,L=128):\n",
        "\n",
        "    v = np.loadtxt(p)\n",
        "\n",
        "    v = np.clip(v,-90,0)\n",
        "\n",
        "    v = (v-v.mean())/(v.std()+1e-8)\n",
        "\n",
        "    v = np.pad(v,(0,max(0,L-len(v))))[:L]\n",
        "\n",
        "    return v.astype(np.float32)\n",
        "\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "\n",
        "    def __init__(self,windows,train=True):\n",
        "\n",
        "        random.shuffle(windows)\n",
        "\n",
        "        s = int(0.8*len(windows))\n",
        "\n",
        "        if train:\n",
        "            self.data = windows[:s]\n",
        "            self.split = \"train\"\n",
        "        else:\n",
        "            self.data = windows[s:]\n",
        "            self.split = \"test\"\n",
        "\n",
        "        self.cache = os.path.join(CACHE,self.split)\n",
        "\n",
        "        os.makedirs(self.cache,exist_ok=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _path(self,i):\n",
        "        return os.path.join(self.cache,f\"{i}.pt\")\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "\n",
        "        p = self._path(i)\n",
        "\n",
        "        if os.path.exists(p):\n",
        "            return torch.load(p)\n",
        "\n",
        "        win,lab = self.data[i]\n",
        "\n",
        "        imgs,sigs=[],[]\n",
        "\n",
        "        for _,r in win.iterrows():\n",
        "\n",
        "            # Image\n",
        "            img = Image.open(r[\"unit1_rgb\"]) \\\n",
        "                      .resize((224,224)) \\\n",
        "                      .convert(\"RGB\")\n",
        "\n",
        "            img = np.array(img)/255.0\n",
        "            imgs.append(img.transpose(2,0,1))\n",
        "\n",
        "            # Signal\n",
        "            sigs.append(load_signal(r[\"unit1_pwr_60ghz\"]))\n",
        "\n",
        "        sample = (\n",
        "            torch.tensor(np.array(imgs),dtype=torch.float32),\n",
        "            torch.tensor(np.array(sigs),dtype=torch.float32),\n",
        "            torch.tensor(lab,dtype=torch.long)\n",
        "        )\n",
        "\n",
        "        torch.save(sample,p)\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "train_ds = CachedDataset(windows,True)\n",
        "test_ds  = CachedDataset(windows,False)\n",
        "\n",
        "\n",
        "def make_loader(ds,shuffle):\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=BATCH,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=WORKERS,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "\n",
        "train_dl = make_loader(train_ds,True)\n",
        "test_dl  = make_loader(test_ds,False)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4. MODELS (FAST)\n",
        "# ============================================================\n",
        "\n",
        "# ---------- ViT (Frozen) ----------\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.vit = timm.create_model(\n",
        "            \"vit_small_patch16_224\",\n",
        "            pretrained=True,\n",
        "            num_classes=0\n",
        "        )\n",
        "\n",
        "        for p in self.vit.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        B,S,C,H,W = x.shape\n",
        "\n",
        "        x = rearrange(x,\"b s c h w -> (b s) c h w\")\n",
        "\n",
        "        f = self.vit(x)\n",
        "\n",
        "        return rearrange(f,\"(b s) d -> b s d\",s=S)\n",
        "\n",
        "\n",
        "# ---------- Signal Transformer ----------\n",
        "\n",
        "class SignalEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self,dim=128):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        layer = nn.TransformerEncoderLayer(\n",
        "            dim,4,batch_first=True\n",
        "        )\n",
        "\n",
        "        self.enc = nn.TransformerEncoder(layer,2)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        return self.enc(x)\n",
        "\n",
        "\n",
        "# ---------- Fusion ----------\n",
        "\n",
        "class Fusion(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.vp = nn.Linear(384,256)\n",
        "        self.sp = nn.Linear(128,256)\n",
        "\n",
        "        self.att = nn.MultiheadAttention(\n",
        "            256,4,batch_first=True\n",
        "        )\n",
        "\n",
        "    def forward(self,v,s):\n",
        "\n",
        "        v = self.vp(v.mean(1)).unsqueeze(1)\n",
        "        s = self.sp(s.mean(1)).unsqueeze(1)\n",
        "\n",
        "        x = torch.cat([v,s],1)\n",
        "\n",
        "        out,att = self.att(x,x,x)\n",
        "\n",
        "        return out.mean(1),att\n",
        "\n",
        "\n",
        "# ---------- Multi-Task Net ----------\n",
        "\n",
        "class MultiTaskNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.vit = ViTEncoder()\n",
        "        self.sig = SignalEncoder()\n",
        "        self.fus = Fusion()\n",
        "\n",
        "        self.h1  = nn.Linear(256,2)\n",
        "        self.h5  = nn.Linear(256,2)\n",
        "        self.h10 = nn.Linear(256,2)\n",
        "\n",
        "    def forward(self,img,sig):\n",
        "\n",
        "        v = self.vit(img)\n",
        "        s = self.sig(sig)\n",
        "\n",
        "        f,att = self.fus(v,s)\n",
        "\n",
        "        return (\n",
        "            self.h1(f),\n",
        "            self.h5(f),\n",
        "            self.h10(f),\n",
        "            att\n",
        "        )\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5. TRAINING (AMP)\n",
        "# ============================================================\n",
        "\n",
        "model = MultiTaskNet().to(DEVICE)\n",
        "\n",
        "opt = torch.optim.AdamW(\n",
        "    list(model.sig.parameters())+\n",
        "    list(model.fus.parameters())+\n",
        "    list(model.h1.parameters())+\n",
        "    list(model.h5.parameters())+\n",
        "    list(model.h10.parameters()),\n",
        "    LR\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "print(\"Training (FAST MODE)...\")\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total = 0\n",
        "\n",
        "    for img,sig,y in tqdm(train_dl):\n",
        "\n",
        "        img = img.to(DEVICE,non_blocking=True)\n",
        "        sig = sig.to(DEVICE,non_blocking=True)\n",
        "        y   = y.to(DEVICE,non_blocking=True)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "\n",
        "            o1,o5,o10,_ = model(img,sig)\n",
        "\n",
        "            loss = (\n",
        "                loss_fn(o1,y[:,0]) +\n",
        "                loss_fn(o5,y[:,1]) +\n",
        "                loss_fn(o10,y[:,2])\n",
        "            )\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "\n",
        "        total += loss.item()\n",
        "\n",
        "    print(f\"Epoch {e+1} | Loss: {total/len(train_dl):.4f}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 6. EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "print(\"Evaluating...\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "YT = {1:[],5:[],10:[]}\n",
        "YP = {1:[],5:[],10:[]}\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    for img,sig,y in test_dl:\n",
        "\n",
        "        img = img.to(DEVICE)\n",
        "        sig = sig.to(DEVICE)\n",
        "\n",
        "        o1,o5,o10,_ = model(img,sig)\n",
        "\n",
        "        YT[1]+=y[:,0].tolist()\n",
        "        YT[5]+=y[:,1].tolist()\n",
        "        YT[10]+=y[:,2].tolist()\n",
        "\n",
        "        YP[1]+=o1.argmax(1).cpu().tolist()\n",
        "        YP[5]+=o5.argmax(1).cpu().tolist()\n",
        "        YP[10]+=o10.argmax(1).cpu().tolist()\n",
        "\n",
        "\n",
        "for h in [1,5,10]:\n",
        "\n",
        "    print(f\"\\n--- Horizon {h} ---\")\n",
        "\n",
        "    print(classification_report(YT[h],YP[h]))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 7. GRAD-CAM (XAI)\n",
        "# ============================================================\n",
        "\n",
        "class GradCAM:\n",
        "\n",
        "    def __init__(self,model):\n",
        "\n",
        "        self.model = model\n",
        "        self.grad = None\n",
        "        self.act  = None\n",
        "\n",
        "        self._register()\n",
        "\n",
        "    def _register(self):\n",
        "\n",
        "        def f_hook(m,i,o):\n",
        "            self.act = o\n",
        "\n",
        "        def b_hook(m,gi,go):\n",
        "            self.grad = go[0]\n",
        "\n",
        "        layer = self.model.vit.vit.blocks[-1]\n",
        "\n",
        "        layer.register_forward_hook(f_hook)\n",
        "        layer.register_backward_hook(b_hook)\n",
        "\n",
        "    def generate(self,img,sig):\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        o1,_,_,_ = self.model(img,sig)\n",
        "\n",
        "        score = o1[:,1].sum()\n",
        "\n",
        "        self.model.zero_grad()\n",
        "        score.backward()\n",
        "\n",
        "        g = self.grad.mean(dim=(2,3),keepdim=True)\n",
        "\n",
        "        cam = (g*self.act).sum(1)\n",
        "\n",
        "        cam = torch.relu(cam)\n",
        "        cam = cam/cam.max()\n",
        "\n",
        "        return cam\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 8. XAI DEMO\n",
        "# ============================================================\n",
        "\n",
        "print(\"Generating XAI...\")\n",
        "\n",
        "cam = GradCAM(model)\n",
        "\n",
        "img,sig,y = test_ds[0]\n",
        "\n",
        "img = img.unsqueeze(0).to(DEVICE)\n",
        "sig = sig.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "heat = cam.generate(img,sig)[0].cpu().numpy()\n",
        "\n",
        "orig = img[0,0].permute(1,2,0).cpu().numpy()\n",
        "\n",
        "heat = cv2.resize(heat,(128,128))\n",
        "heat = cv2.applyColorMap(\n",
        "    np.uint8(255*heat),\n",
        "    cv2.COLORMAP_JET\n",
        ")\n",
        "\n",
        "out = heat*0.4 + orig*255\n",
        "\n",
        "plt.imshow(out.astype(np.uint8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Explainable AI (Grad-CAM)\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
